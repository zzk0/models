name: text_cnn_bert
embedding:
  type: bert
  name: bert-base-cased
  dimension: 768
  use_local: true
  path: ./pretrained/bert-base-cased
max_seq_len: 128
dataset:
  name: imdb
  path: ./data/aclImdb
train:
  accelerator: gpu
  accelerator_devices: 2
  epochs: 10
  num_workers: 8
  train_batch_size: 128
  dev_batch_size: 128
  test_batch_size: 128
  precision: 16
optimizer:
  lr: 3e-5  #  warning: a large learning rate will break the parameter of BERT
model:
  dropout: 0.5
  num_filters: 256
  filter_sizes: [2, 3, 4]
  num_classes: 2
log_path: ./logs
log_every_n_steps: 10
sqlite_path: ./database/ml.db
