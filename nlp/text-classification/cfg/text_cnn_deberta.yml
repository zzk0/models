name: text_cnn_deberta
embedding:
  type: bert
  name: microsoft/deberta-v3-base
  dimension: 768
  use_local: true
  path: ./pretrained/deberta-v3-base
max_seq_len: 128
dataset:
  name: imdb
  path: ./data/aclImdb
train:
  accelerator: gpu
  accelerator_devices: 2
  epochs: 20
  num_workers: 8
  train_batch_size: 128
  dev_batch_size: 128
  test_batch_size: 128
optimizer:
  lr: 3e-5  #  warning: a large learning rate will break the parameter of BERT
model:
  dropout: 0.5
  num_filters: 256
  filter_sizes: [2, 3, 4]
  num_classes: 2
log_path: ./logs
log_every_n_steps: 10
